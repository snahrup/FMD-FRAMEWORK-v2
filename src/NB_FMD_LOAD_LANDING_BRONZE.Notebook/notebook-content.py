# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "cf57e8bf-7b34-471b-adea-ed80d05a4fdb",
# META       "default_lakehouse_name": "LH_BRONZE_LAYER",
# META       "default_lakehouse_workspace_id": "a3a180ff-fbc2-48fd-a65f-27ae7bb6709a",
# META       "known_lakehouses": [
# META         {
# META           "id": "cf57e8bf-7b34-471b-adea-ed80d05a4fdb"
# META         },
# META         {
# META           "id": "2aef4ede-2918-4a6b-8ec6-a42108c67806"
# META         }
# META       ]
# META     },
# META     "environment": {}
# META   }
# META }

# MARKDOWN ********************

# # FMD Load Landing Zone to Bronze Notebook
# 
# ## Overview
# This notebook handles the data loading process from the Landing Zone to the Bronze layer in the FMD framework. It processes source files, applies data quality checks, performs cleansing, and loads data into Bronze Delta tables.
# 
# ## Key Features
# - **Source File Validation**: Checks if source files exist before processing
# - **Data Quality Checks**: Validates primary keys and detects duplicates
# - **Cleansing Rules**: Applies configurable cleansing rules from the framework database
# - **Change Detection**: Uses hash columns to detect changes in data
# - **Incremental Loading**: Supports both full and incremental load patterns
# - **Audit Logging**: Tracks execution details in the framework database
# - **Delta Lake Integration**: Writes data to Delta tables with optimization settings
# 
# ## Process Flow
# 1. Load libraries and configuration settings
# 2. Set up audit logging and database connections
# 3. Read source file from Landing Zone (Parquet/CSV)
# 4. Perform data quality checks (PK validation, duplicate detection)
# 5. Apply cleansing rules from framework configuration
# 6. Add hash columns for change tracking
# 7. Execute incremental or full load to Bronze Delta table
# 8. Update processing status and complete audit logging


# CELL ********************

config_settings=notebookutils.variableLibrary.getLibrary("VAR_CONFIG_FMD")
default_settings=notebookutils.variableLibrary.getLibrary("VAR_FMD")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# PARAMETERS CELL ********************

# Set arguments
PrimaryKeys = ""
SourceFileType='parquet'
IsIncremental = False

SourceWorkspace= ''
SourceLakehouse = ''
SourceLakehouseName = ''
SourceFilePath = ''
SourceFileName = ''
DataSourceNamespace = ''

TargetWorkspace = ''
TargetLakehouse = ''
TargetLakehouseName = ''
TargetSchema = ''
TargetName = ''

LandingzoneEntityId =""
BronzeLayerEntityId =""
# # CSV
CompressionType = 'infer'
ColumnDelimiter = ','
RowDelimiter = '\n'
EscapeCharacter = '"'
Encoding = 'UTF-8'
first_row_is_header = True
infer_schema = True
key_vault =default_settings.key_vault_uri_name
cleansing_rules = []

###############################Logging Parameters###############################
driver = '{ODBC Driver 18 for SQL Server}'
connstring=config_settings.fmd_fabric_db_connection
database=config_settings.fmd_fabric_db_name
schema_enabled =default_settings.lakehouse_schema_enabled
EntityLayer='Bronze'
result_data=''

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Load Libraries

# CELL ********************

import re
from datetime import datetime, timezone
import json
from delta.tables import *
from pyspark.sql.functions import sha2, concat_ws, md5, StringType,current_timestamp

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Define Starttime

# CELL ********************

start_audit_time = datetime.now()


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

token =  notebookutils.credentials.getToken('https://analysis.windows.net/powerbi/api')

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Execution Logic

# CELL ********************

%run NB_FMD_UTILITY_FUNCTIONS

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Define Stored Procedures for Logging

# CELL ********************

# Ensure TriggerTime is formatted correctly
TriggerTime = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
notebook_name=  notebookutils.runtime.context['currentNotebookName']


UpsertPipelineLandingzoneEntity = (
    f"[execution].[sp_UpsertPipelineLandingzoneEntity] "
    f"@Filename = \"{SourceFileName}\", "
    f"@FilePath = \"{SourceFilePath}\", "
    f"@IsProcessed = \"True\", "
    f"@LandingzoneEntityId = \"{LandingzoneEntityId}\""
)

InsertPipelineBronzeLayerEntity = (
    f"[execution].[sp_UpsertPipelineBronzeLayerEntity] "
    f"@SchemaName = \"{TargetSchema}\", "
    f"@TableName = \"{TargetName}\", "
    f"@IsProcessed = \"False\", "
    f"@BronzeLayerEntityId = \"{BronzeLayerEntityId}\""
)

StartNotebookActivity = (
    f"[logging].[sp_AuditNotebook] "
    f"@NotebookGuid = \"{NotebookExecutionId}\", "
    f"@NotebookName = \"{notebook_name}\", "
    f"@PipelineRunGuid = \"{PipelineRunGuid}\", "
    f"@PipelineParentRunGuid = \"{PipelineParentRunGuid}\", "
    f"@NotebookParameters = \"{TargetName}\", "
    f"@TriggerType = \"{TriggerType}\", "
    f"@TriggerGuid = \"{TriggerGuid}\", "
    f"@TriggerTime = \"{TriggerTime}\", "
    f"@LogData = '{{\"Action\":\"Start\"}}', "
    f"@LogType = \"StartNotebookActivity\", "
    f"@WorkspaceGuid = \"{SourceWorkspace}\", "
    f"@EntityId = \"{BronzeLayerEntityId}\", "
    f"@EntityLayer = \"{EntityLayer}\""
)

EndNotebookActivity = (
    f"[logging].[sp_AuditNotebook] "
    f"@NotebookGuid = \"{NotebookExecutionId}\", "
    f"@NotebookName = \"{notebook_name}\", "
    f"@PipelineRunGuid = \"{PipelineRunGuid}\", "
    f"@PipelineParentRunGuid = \"{PipelineParentRunGuid}\", "
    f"@NotebookParameters = \"{TargetName}\", "
    f"@TriggerType = \"{TriggerType}\", "
    f"@TriggerGuid = \"{TriggerGuid}\", "
    f"@TriggerTime = \"{TriggerTime}\", "
    f"@LogType = \"EndNotebookActivity\", "
    f"@WorkspaceGuid = \"{SourceWorkspace}\", "
    f"@EntityId = \"{BronzeLayerEntityId}\", "
    f"@EntityLayer = \"{EntityLayer}\""
)
GetCleansingRule = (
    f"[execution].[sp_GetBronzeCleansingRule]"
    f"@BronzeLayerEntityId = \"{BronzeLayerEntityId}\""
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Set Configuration

# CELL ********************

execute_with_outputs(StartNotebookActivity, driver, connstring, database)
_processing_error = None  # Global error flag — if set, downstream cells skip and we log the error at exit

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

#Make sure you have disabled V-Order, Bronze we want to load fast

spark.conf.set("sprk.sql.parquet.vorder.enabled", "false")

spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.datetimeRebaseModeInWrite", "CORRECTED")

spark.conf.set('spark.microsoft.delta.optimize.fast.enabled', True)
spark.conf.set('spark.microsoft.delta.optimize.fileLevelTarget.enabled', True)
spark.conf.set('spark.databricks.delta.autoCompact.enabled', True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Set your loading paths

# CELL ********************

#Set SourceFile and target Location
if schema_enabled == True:
    source_changes_data_path = f"abfss://{SourceWorkspace}@onelake.dfs.fabric.microsoft.com/{SourceLakehouse}/Files/{SourceFilePath}/{SourceFileName}"
    target_data_path = f"abfss://{TargetWorkspace}@onelake.dfs.fabric.microsoft.com/{TargetLakehouse}/Tables/{DataSourceNamespace}/{TargetSchema}_{TargetName}"
elif schema_enabled != True:
    source_changes_data_path = f"abfss://{SourceWorkspace}@onelake.dfs.fabric.microsoft.com/{SourceLakehouse}/Files/{SourceFilePath}/{SourceFileName}"
    target_data_path = f"abfss://{TargetWorkspace}@onelake.dfs.fabric.microsoft.com/{TargetLakehouse}/Tables/{DataSourceNamespace}_{TargetSchema}_{TargetName}"


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Load new from Data Landingzone

# CELL ********************

if _processing_error is None:
    # Check file existence using Spark (more robust than notebookutils.fs.exists for cross-workspace access)
    _file_exists = False
    try:
        _file_exists = notebookutils.fs.exists(source_changes_data_path)
    except Exception as _e:
        print(f"notebookutils.fs.exists failed: {_e}")
        _file_exists = False

    # Fallback: try reading with Spark directly if notebookutils says file doesn't exist
    if not _file_exists:
        try:
            _test_df = spark.read.format(SourceFileType).option("header","true").load(source_changes_data_path)
            _test_count = _test_df.limit(1).count()
            if _test_count > 0:
                _file_exists = True
                print(f"File found via Spark (notebookutils.fs.exists returned False): {source_changes_data_path}")
        except Exception as _e2:
            print(f"Spark read also failed: {_e2}")
            _file_exists = False

    if not _file_exists:
        print(f"FILE NOT FOUND: {source_changes_data_path}")
        execute_with_outputs(UpsertPipelineLandingzoneEntity, driver, connstring, database)
        TotalRuntime = str((datetime.now() - start_audit_time))
        end_audit_time =  str(datetime.now())
        start_audit_time =str(start_audit_time)
        result_data = {
        "Action" : "End", "CopyOutput":{
            "Total Runtime": TotalRuntime,
            "TargetSchema": TargetSchema,
            "TargetName" : TargetName,
            "SourceFilePath" : SourceFilePath,
            "SourceFileName" : 'FILE NOT FOUND',
            "LandingzoneEntityId" : LandingzoneEntityId,
            "EntityId" : BronzeLayerEntityId,
            "StartTime" : start_audit_time,
            "EndTime" : end_audit_time

        }
        }
        execute_with_outputs(EndNotebookActivity, driver, connstring, database, LogData=json.dumps(result_data))

        notebookutils.notebook.exit("FILE_NOT_FOUND")

    # Read all incoming changes in Parquet format
    try:
        dfDataChanged= spark.read\
                        .format(SourceFileType) \
                        .option("header","true") \
                        .load(f"{source_changes_data_path}")
    except Exception as _e:
        _processing_error = f"Spark read failed for {SourceFileName}: {_e}"
        print(f"ERROR: {_processing_error}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

if _processing_error is None:
    # Replace spaces with underscores in column names
    new_columns = [col.replace(' ', '') for col in dfDataChanged.columns]
    # Rename the columns
    dfDataChanged = dfDataChanged.toDF(*new_columns)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## DQ Checks

# CELL ********************

if _processing_error is None:
    try:
        #split PKcolumns string on , ; or :
        PrimaryKeys = str(PrimaryKeys)

        PrimaryKeys = re.split('[, ; :]', PrimaryKeys)
        #remove potential whitespaces around Pk columns
        PrimaryKeys = [column.strip() for column in PrimaryKeys if column != ""]

        # Handle "N/A" or empty PKs — fall back to using ALL columns as composite key
        if not PrimaryKeys or PrimaryKeys == ["N/A"] or PrimaryKeys == ["n/a"] or PrimaryKeys == [""]:
            print(f"WARNING: PrimaryKeys is '{PrimaryKeys}' — using all columns as composite key for {TargetName}")
            key_columns = list(dfDataChanged.columns)
        else:
            key_columns = PrimaryKeys
            # Check if all PK's exist in source
            for pk_column in key_columns:
                if pk_column not in dfDataChanged.columns:
                    # Log warning but don't crash — remove missing PKs
                    print(f"WARNING: PK column '{pk_column}' not found in source for {TargetName}. Available: {dfDataChanged.columns[:10]}...")
                    key_columns = [k for k in key_columns if k in dfDataChanged.columns]

            # If no valid PK columns remain, fall back to all columns
            if not key_columns:
                print(f"WARNING: No valid PK columns found for {TargetName} — using all columns as composite key")
                key_columns = list(dfDataChanged.columns)

        read_key_columns = [column for column in dfDataChanged.columns if column in key_columns]

        # Add a column with the calculated hash, easier in later stage of with multiple PK
        dfDataChanged = (dfDataChanged
                        .withColumn("HashedPKColumn", sha2(concat_ws("||", *read_key_columns), 256)))
    except Exception as _e:
        _processing_error = f"PK validation failed for {TargetName}: {_e}"
        print(f"ERROR: {_processing_error}")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Check for Duplicates

# CELL ********************

if _processing_error is None:
    try:
        if dfDataChanged.select('HashedPKColumn').distinct().count() != dfDataChanged.select('HashedPKColumn').count():
            print(f'WARNING: Source file contains duplicated rows for PK: {", ".join(key_columns)} in {TargetName}. Deduplicating...')
            dfDataChanged = dfDataChanged.dropDuplicates(['HashedPKColumn'])
    except Exception as _e:
        _processing_error = f"Duplicate check failed for {TargetName}: {_e}"
        print(f"ERROR: {_processing_error}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Perform Cleansing

# CELL ********************

if _processing_error is None:
    if cleansing_rules == "":
        cleansing_rules = []

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

if _processing_error is None:
    try:
        CleansingRules=execute_with_outputs(GetCleansingRule, driver, connstring, database)
        rules_str = None
        # Extract the string
        rules_str = CleansingRules["result_sets"][0][0]["CleansingRules"]
        if rules_str != None :
        # Convert JSON text → Python dict/list
            cleansing_rules = json.loads(rules_str)
    except Exception as _e:
        print(f"WARNING: Cleansing rules fetch failed for {TargetName}: {_e}. Continuing without cleansing.")
        cleansing_rules = []

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

%run NB_FMD_DQ_CLEANSING

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

if _processing_error is None:
    try:
        dfDataChanged=handle_cleansing_functions(dfDataChanged,cleansing_rules)
    except Exception as _e:
        print(f"WARNING: Cleansing application failed for {TargetName}: {_e}. Continuing without cleansing.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Add Hash

# CELL ********************

if _processing_error is None:
    try:
        non_key_columns = [column for column in dfDataChanged.columns if column not in key_columns]

        #add a hashed cloumn to detect changes
        dfDataChanged = dfDataChanged.withColumn("HashedNonKeyColumns", md5(concat_ws("||", *non_key_columns).cast(StringType())))

        #Add RecordLoadDate to see when the record arrived
        dfDataChanged = dfDataChanged.withColumn('RecordLoadDate', current_timestamp())
    except Exception as _e:
        _processing_error = f"Hash column creation failed for {TargetName}: {_e}"
        print(f"ERROR: {_processing_error}")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Read Original if exists

# CELL ********************

if _processing_error is None:
    try:
        #Check if Target exist, if exists read the original data if not create table and exit
        if DeltaTable.isDeltaTable(spark, target_data_path):
            # Read original/current data
            dfDataOriginal = (spark
                                .read.format("delta")
                                .load(target_data_path)
                                )

        else:
            # Use first load when no data exists yet and then exit
            dfDataChanged.write.format("delta").mode("overwrite").save(target_data_path)
            TotalRuntime = str((datetime.now() - start_audit_time))
            TotalRuntime = str((datetime.now() - start_audit_time))
            end_audit_time =  str(datetime.now())
            start_audit_time =str(start_audit_time)
            # Your data
            result_data = {
                "Action" : "End", "CopyOutput":{
                    "Total Runtime": TotalRuntime,
                    "TargetSchema": TargetSchema,
                    "TargetName" : TargetName,
                    "SourceFilePath" : SourceFilePath,
                    "SourceFileName" : SourceFileName,
                    "LandingzoneEntityId" : LandingzoneEntityId,
                    "EntityId" : BronzeLayerEntityId,
                    "StartTime" : start_audit_time,
                    "EndTime" : end_audit_time

                }
                }

            execute_with_outputs(UpsertPipelineLandingzoneEntity, driver, connstring, database)
            execute_with_outputs(InsertPipelineBronzeLayerEntity, driver, connstring, database)
            execute_with_outputs(EndNotebookActivity, driver, connstring, database, LogData=json.dumps(result_data))
            notebookutils.notebook.exit("OK")
    except Exception as _e:
        _processing_error = f"Delta table read/write failed for {TargetName}: {_e}"
        print(f"ERROR: {_processing_error}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Merge table

# CELL ********************

if _processing_error is None:
    try:
        #merge table
        deltaTable = DeltaTable.forPath(spark, f'{target_data_path}')
        if IsIncremental in [False, 'false', 'False']:
            merge = deltaTable.alias('original') \
                .merge(dfDataChanged.alias('updates'), 'original.HashedPKColumn == updates.HashedPKColumn') \
                .whenNotMatchedInsertAll() \
                .whenMatchedUpdateAll('original.HashedNonKeyColumns != updates.HashedNonKeyColumns') \
                .whenNotMatchedBySourceDelete() \
                .execute()
        elif IsIncremental not in [False, 'false', 'False']:
            merge = deltaTable.alias('original') \
                .merge(dfDataChanged.alias('updates'), 'original.HashedPKColumn == updates.HashedPKColumn') \
                .whenNotMatchedInsertAll() \
                .whenMatchedUpdateAll('original.HashedNonKeyColumns != updates.HashedNonKeyColumns') \
                .execute()
    except Exception as _e:
        _processing_error = f"Delta merge failed for {TargetName}: {_e}"
        print(f"ERROR: {_processing_error}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Define Results

# CELL ********************

TotalRuntime = str((datetime.now() - start_audit_time))
end_audit_time =  str(datetime.now())
start_audit_time =str(start_audit_time)

if _processing_error is not None:
    # Error path — log the error and exit with ERROR status
    result_data = {
        "Action" : "End", "CopyOutput":{
            "Total Runtime": TotalRuntime,
            "TargetSchema": TargetSchema,
            "TargetName" : TargetName,
            "SourceFilePath" : SourceFilePath,
            "SourceFileName" : SourceFileName,
            "LandingzoneEntityId" : LandingzoneEntityId,
            "EntityId" : BronzeLayerEntityId,
            "StartTime" : start_audit_time,
            "EndTime" : end_audit_time,
            "Error" : str(_processing_error)[:500]
        }
    }
    print(f"ENTITY FAILED: {TargetName} — {_processing_error}")
else:
    # Success path
    result_data = {
        "Action" : "End", "CopyOutput":{
            "Total Runtime": TotalRuntime,
            "TargetSchema": TargetSchema,
            "TargetName" : TargetName,
            "SourceFilePath" : SourceFilePath,
            "SourceFileName" : SourceFileName,
            "LandingzoneEntityId" : LandingzoneEntityId,
            "EntityId" : BronzeLayerEntityId,
            "StartTime" : start_audit_time,
            "EndTime" : end_audit_time
        }
    }


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Logging and update queue

# CELL ********************

try:
    execute_with_outputs(UpsertPipelineLandingzoneEntity, driver, connstring, database)
    execute_with_outputs(InsertPipelineBronzeLayerEntity, driver, connstring, database)
    execute_with_outputs(EndNotebookActivity, driver, connstring, database, LogData=json.dumps(result_data))
except Exception as _log_err:
    print(f"WARNING: Final logging failed for {TargetName}: {_log_err}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# ## Notebook exit

# CELL ********************

if _processing_error is not None:
    notebookutils.notebook.exit(f"ERROR: {str(_processing_error)[:200]}")
else:
    notebookutils.notebook.exit("OK")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
